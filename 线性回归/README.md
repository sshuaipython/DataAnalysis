## 01.使用梯度下降方法实现线性回归(demo01_bgd.py)

### 过程

1. 读取训练集的输入数据和输出数据
2. 设定初始模型:y=x+1
3. 根据预测函数y=w0+w1x，记录此次迭代损失值
4. 求损失函数关于w0和w1的偏导数，计算产生下一次迭代的w0和w1
5. 迭代循环1000次，重复3、4步骤
6. 用训练集作为测试集计算实际的预测输出，和实际输出比较
  (5.0 -> 5.197342694662536,
  5.5 -> 5.423697847192317,
  6.0 -> 5.876408152251877,
  6.8 -> 6.5554736098412185,
  7.0 -> 7.2345390674305605)
  
7. 绘制回归曲线，测试值(训练值)和预测值的散点图
![](https://github.com/silencesong/DataAnalysis/blob/master/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/Images/myplot.png)

8. 可视化梯度下降过程中的w0,w1和loss的训练过程
![](https://github.com/silencesong/DataAnalysis/blob/master/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/Images/myplot2.png)

9. 可视化损失函数曲面

![](https://github.com/silencesong/DataAnalysis/blob/master/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/Images/myplot3.png)

### 结论：
1. 从可视化图中可以看出，使用梯度下降的计算方法，迭代1000次后，损失值loss降为0.08742142。
2. 从w0,w1和loss的训练过程中可以看出，前20次迭代loss下降频率很快，之后下降趋于平缓
3. 可以从损失函数曲面上看出梯度下降的路线，损失值越来越小

## 02.调用sklearn中的线性回归模型拟合数据(demo02_line.py)

### 过程

1. 读取训练输入数据和输出数据
2. 创建线性回归器，训练模型，预测数据
3. 得到评估结果：平均绝对值误差0.54828，平均平方误差0.43607，中位绝对值误差中位绝对值误差，R2得分0.73626
4. 可视化回归曲线和训练数据的散点图

![](https://github.com/silencesong/DataAnalysis/blob/master/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/Images/myplot4.png)

### 结论

使用线性回归模型的R2得分为0.73626，得分不是很高，散点分布在回归线周围，能看出数据的变化趋势具有线性增长的特性

## 03.使用多项式特征扩展器和线性回归模型做多项式回归(demo03_poly.py)

### 过程

1. 读取训练输入数据和输出数据
2. 将一个多项式特征扩展预处理器和一个线性回归器串联为一个管线
2. 用已知的输入和输出来训练该回归器，根据给定的输入来预测对应的输出
3. 设定最高次幂为10的多项式回归模型，得到评估结果：R2得分为0.78686
4. 可视化回归曲线和训练数据的散点图

![](https://github.com/silencesong/DataAnalysis/blob/master/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/Images/myplot5.png)

### 结论

1. 本案例中使用的10次多项式回归模型的R2得分为0.78686，得分较线性回归有所上升。
2. 如果增大太多多项式的最高次幂，会出现过拟合现象。
3. 因此在多项式回归模型中，为了避免出现欠拟合和过拟合的现象，需要选择好多项式的次幂数。
